{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/argennof/VisCompu2022/blob/main/Lab_5_Semantic_and_Instance_Segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AT1WjbFzVFnB"
      },
      "source": [
        "# Semantic Segmentation and Instance Segmentation\n",
        "\n",
        "**NOTA**: Este notebook ha sido creado a partir de diferentes recursos disponibles en la web, los cuales se listan a continuación.\n",
        "- https://d2l.ai/chapter_computer-vision/semantic-segmentation-and-dataset.html\n",
        "- https://www.tensorflow.org/tutorials/images/segmentation?hl=en\n",
        "- https://nanonets.com/blog/how-to-do-semantic-segmentation-using-deep-learning/\n",
        "- https://nanonets.com/blog/semantic-image-segmentation-2020/\n",
        "- https://towardsdatascience.com/review-fcn-semantic-segmentation-eb8c9b50d2d1\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Semantic Segmentation\n",
        "\n",
        "La segmentación semántica intenta **segmentar imágenes** en regiones con diferentes **categorías semánticas**. Estas regiones semánticas se usan para etiquetar y predecir objetos a nivel de píxeles. La siguiente figura muestra una imagen segmentada semánticamente, con áreas etiquetadas para \"Perro\", \"Gato\", y \"Background\". \n",
        "\n",
        "![](https://d2l.ai/_images/segmentation.svg)\n",
        "\n",
        "Por lo tanto, la segmentación semántica trata de descubrir con precisión el límite exacto de los objetos en la imagen diferenciandose del resto de las tareas más habituales de computer vision.\n"
      ],
      "metadata": {
        "id": "HPht8sphpAim"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instance Segmentation\n",
        "\n",
        "Por otro lado, la **segmentación de instancias** (también llamada detección y segmentación simultánea) se enfoca en reconocer las regiones a nivel de píxeles de cada instancia de objeto en una imagen. A diferencia de la segmentación semántica, la segmentación de instancias necesita distinguir no solo la semántica, sino también diferentes instancias de objetos. Por ejemplo, si hay dos perros en la imagen, la segmentación de instancias debe distinguir a cuál de los dos perros pertenece un píxel.\n",
        "\n",
        "![](https://nanonets.com/blog/content/images/size/w1000/2020/08/59b6d0529299e.png)"
      ],
      "metadata": {
        "id": "hpd8gQ74nQEV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrZlfZv9EHSZ"
      },
      "source": [
        "# Semantic Segmentation con CNNs\n",
        "\n",
        "En general, las CNNs diseñadas para la tarea de segmentación semántica presentan una arquitectura que constan de dos partes esenciales: un **encoder** y un **decoder**. \n",
        "\n",
        "![](https://www.mdpi.com/applsci/applsci-07-00312/article_deploy/html/images/applsci-07-00312-g002.png)\n",
        "\n",
        "El **encoder** consiste en una serie de *capas convolucionales* y *capas de pooling* (o submuestreo) que se encargan de la extracción automática de features. Como ya vimos en notebooks anteriores, el propósito del pooling es reducir la resolución de los features maps a medida que se avanza en la red. Esto permite mejorar la robustez del clasificador, eliminando features redundantes y haciendo al algoritmo invariante espacialmente. Por lo tanto, el propósito principal del decoder es obtener una *representación abstracta* de baja resolución la imagen.\n",
        "\n",
        "Por otro lado, el **decoder** se encarga de *proyectar semánticamente* los feature maps de baja resolución aprendidas por el encoder en una *máscara de segmentación*, esto es, una imagen de igual resolución que la imagen de entrada que representa la clasificación densa por píxeles. Dado que los features maps del encoder sufren una pérdida de resolución espacial, el decoder utiliza una serie de [*capas de upsampling*](https://www.oreilly.com/library/view/deep-learning-for/9781788295628/467cf02b-dc52-49c5-9289-b2721f6758da.xhtml) (lo contrario al pooling) y [*capas de convolución transpuesta*](https://www.oreilly.com/library/view/hands-on-convolutional-neural/9781789130331/c6c4a7a2-7776-454a-83c5-3779ab807a00.xhtml) (también llamadas de-convoluciones) para proyectar las features maps del encoder al espacio de píxeles. Por último, la *capa softmax* es la encargada de realizar la clasificación de píxeles. \n",
        "\n",
        "En este notebook vamos a utilizar una arquitectura (encoder y decoder) inspirada en las **[Fully Convolutional Networks (FCN)](https://arxiv.org/pdf/1411.4038.pdf)**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkB2WTlkQLpr"
      },
      "source": [
        "## Fully Convolutional Network: de clasificación de imágenes a segmentación semántica.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMP7mglMuGT2"
      },
      "source": [
        "Este turial se enfoca en la tarea de segmentación semántica, usando una versión modificada de la arquitectura [U-Net](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/).\n",
        "\n",
        "![](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a instalar e importar las depedencias necesarias."
      ],
      "metadata": {
        "id": "d6F5xmgV3Sqs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQmKthrSBCld"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/tensorflow/examples.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQX7R4bhZy5h"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g87--n2AtyO_"
      },
      "outputs": [],
      "source": [
        "from tensorflow_examples.models.pix2pix import pix2pix\n",
        "\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWe0_rQM4JbC"
      },
      "source": [
        "## Descargarmos el Oxford-IIIT Pets dataset\n",
        "\n",
        "Este tutorial utiliza el [Oxford-IIIT Pet Dataset](https://www.robots.ox.ac.uk/~vgg/data/pets/) ([Parkhi et al, 2012](https://www.robots ox.ac.uk/~vgg/publications/2012/parkhi12a/parkhi12a.pdf)). Este dataset consta de imágenes de 37 razas de mascotas, con 200 imágenes por raza (~100 cada una en los splits de train y test). Cada imagen incluye las etiquetas correspondientes y las máscaras de píxeles. Las máscaras son etiquetas de clase para cada píxel. A cada píxel se le asigna una de tres categorías:\n",
        "\n",
        "- Clase 1: Píxel perteneciente a la mascota.\n",
        "- Clase 2: Píxel bordeando a la mascota.\n",
        "- Clase 3: ninguna de las anteriores/un píxel circundante.\n",
        "\n",
        "Este dataset esta disponibe en el repositorio de [TensorFlow Datasets](https://www.tensorflow.org/datasets/catalog/oxford_iiit_pet). Las máscaras de segmentación son incluídas en la versión 3+ del dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40ITeStwDwZb"
      },
      "outputs": [],
      "source": [
        "dataset, info = tfds.load('oxford_iiit_pet:3.*.*', with_info=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJcVdj_U4vzf"
      },
      "source": [
        "> **NOTA IMPORTANTE**: Las celdas que siguen a continuación definen algunas funciones utilitarias y establecen un pipeline para el preprocesamiento de las imágenes del dataset, para ajustarlo a los requerimientos del proceso de entrenamiento y validación del algoritmo.** Se dejan breves explicaciones sobre que hace cada celda, pero no es el objetivo de este notebook profundizar en esas temáticas**. Pueden simplemente asumir que que estás celdas hacen su trabajo y avanzar.\n",
        "\n",
        "Primero vamos a definir una función para normalizar los datos y un función para cargar las imágenes.\n",
        "\n",
        "La función `normalize()` normaliza los valores de color de la imagen al rango `[0, 1]`. Además, siendo que los píxeles de las máscaras de segmentación están etiquetados como clase {1, 2, 3}, por conveniencia restamos 1 a las mismas, lo que da como resultado etiquetas: {0, 1, 2}."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FD60EbcAQqov"
      },
      "outputs": [],
      "source": [
        "def normalize(input_image, input_mask):\n",
        "  input_image = tf.cast(input_image, tf.float32) / 255.0\n",
        "  input_mask -= 1\n",
        "  return input_image, input_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zf0S67hJRp3D"
      },
      "outputs": [],
      "source": [
        "def load_image(datapoint):\n",
        "  input_image = tf.image.resize(datapoint['image'], (128, 128))\n",
        "  input_mask = tf.image.resize(datapoint['segmentation_mask'], (128, 128))\n",
        "\n",
        "  input_image, input_mask = normalize(input_image, input_mask)\n",
        "\n",
        "  return input_image, input_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65-qHTjX5VZh"
      },
      "source": [
        "El dataset ya contiene los splits de para entrenamiento y testeo requeridos, por lo que usaremos esas mismas divisiones:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHwj2-8SaQli"
      },
      "outputs": [],
      "source": [
        "TRAIN_LENGTH = info.splits['train'].num_examples\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 1000\n",
        "STEPS_PER_EPOCH = TRAIN_LENGTH // BATCH_SIZE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39fYScNz9lmo"
      },
      "outputs": [],
      "source": [
        "train_images = dataset['train'].map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "test_images = dataset['test'].map(load_image, num_parallel_calls=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9hGHyg8L3Y1"
      },
      "source": [
        "La siguiente clase realiza un proceso de *augmentations*, volteando aleatoriamente una imagen. En el tutorial [Image augmentation](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/images/data_augmentation.ipynb) pueden obtener más información sobre este proceso.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUWdDJRTL0PP"
      },
      "outputs": [],
      "source": [
        "class Augment(tf.keras.layers.Layer):\n",
        "  def __init__(self, seed=42):\n",
        "    super().__init__()\n",
        "    # both use the same seed, so they'll make the same random changes.\n",
        "    self.augment_inputs = tf.keras.layers.RandomFlip(mode=\"horizontal\", seed=seed)\n",
        "    self.augment_labels = tf.keras.layers.RandomFlip(mode=\"horizontal\", seed=seed)\n",
        "  \n",
        "  def call(self, inputs, labels):\n",
        "    inputs = self.augment_inputs(inputs)\n",
        "    labels = self.augment_labels(labels)\n",
        "    return inputs, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTIbNIBdcgL3"
      },
      "source": [
        "Creamos un pipeline de entrada para las imágenes de entrenamiento y testeo:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPscskQcNCx4"
      },
      "outputs": [],
      "source": [
        "train_batches = (\n",
        "    train_images\n",
        "    .cache()\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .repeat()\n",
        "    .map(Augment())\n",
        "    .prefetch(buffer_size=tf.data.AUTOTUNE))\n",
        "\n",
        "test_batches = test_images.batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xa3gMAE_9qNa"
      },
      "source": [
        "Definimos una función para visualizar una imagen y su máscara correspondiente en el dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3N2RPAAW9q4W"
      },
      "outputs": [],
      "source": [
        "def display(display_list):\n",
        "  plt.figure(figsize=(15, 15))\n",
        "\n",
        "  title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
        "\n",
        "  for i in range(len(display_list)):\n",
        "    plt.subplot(1, len(display_list), i+1)\n",
        "    plt.title(title[i])\n",
        "    plt.imshow(tf.keras.utils.array_to_img(display_list[i]))\n",
        "    plt.axis('off')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6u_Rblkteqb"
      },
      "outputs": [],
      "source": [
        "for images, masks in train_batches.take(2):\n",
        "  sample_image, sample_mask = images[0], masks[0]\n",
        "  display([sample_image, sample_mask])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAOe93FRMk3w"
      },
      "source": [
        "## El modelo\n",
        "\n",
        "El modelo que se utiliza aquí es una [U-Net](https://arxiv.org/abs/1505.04597) modificado. Una U-Net consta de un encoder (downsampler) y un decoder (upsampler). Para aprender features sólidas y reducir la cantidad de parámetros entrenables, se usa como encoder un modelo previamente entrenado: [MobileNetV2](https://arxiv.org/abs/1801.04381). Para el decoder se utilizará el bloque *upsample* previamente implementado en el ejemplo [pix2pix](https://github.com/tensorflow/examples/blob/master/tensorflow_examples/models/pix2pix/pix2pix.py) en el repositorio de ejemplos de TensorFlow.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4mQle3lthit"
      },
      "source": [
        "Para conseguir el modelo MobileNetV2 preentrenado vamos a utilizar `tf.keras.applications`, un módulo de TensorFlow que nos ofrece un amplio repositorio de diferentes arquitecturas con sus pesos preentrenados. Tener en cuenta que el encoder no se entrenará durante el proceso de entrenamiento, es decir, sus pesos no se van a actualizar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "liCeLH0ctjq7"
      },
      "outputs": [],
      "source": [
        "base_model = tf.keras.applications.MobileNetV2(input_shape=[128, 128, 3], include_top=False)\n",
        "\n",
        "# Use the activations of these layers\n",
        "layer_names = [\n",
        "    'block_1_expand_relu',   # 64x64\n",
        "    'block_3_expand_relu',   # 32x32\n",
        "    'block_6_expand_relu',   # 16x16\n",
        "    'block_13_expand_relu',  # 8x8\n",
        "    'block_16_project',      # 4x4\n",
        "]\n",
        "base_model_outputs = [base_model.get_layer(name).output for name in layer_names]\n",
        "\n",
        "# Create the feature extraction model\n",
        "down_stack = tf.keras.Model(inputs=base_model.input, outputs=base_model_outputs)\n",
        "\n",
        "down_stack.trainable = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPw8Lzra5_T9"
      },
      "source": [
        "The decoder/upsampler is simply a series of upsample blocks implemented in TensorFlow examples:\n",
        "\n",
        "El decoder/upsampler es simplemente una serie de bloques de muestreo ascendente (upsamples) implementados en los ejemplos de TensorFlow:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0ZbfywEbZpJ"
      },
      "outputs": [],
      "source": [
        "up_stack = [\n",
        "    pix2pix.upsample(512, 3),  # 4x4 -> 8x8\n",
        "    pix2pix.upsample(256, 3),  # 8x8 -> 16x16\n",
        "    pix2pix.upsample(128, 3),  # 16x16 -> 32x32\n",
        "    pix2pix.upsample(64, 3),   # 32x32 -> 64x64\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finalmente ensamblamos todo en una sola arquitectura:"
      ],
      "metadata": {
        "id": "f4U4wyKism7P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45HByxpVtrPF"
      },
      "outputs": [],
      "source": [
        "def unet_model(output_channels:int):\n",
        "  inputs = tf.keras.layers.Input(shape=[128, 128, 3])\n",
        "\n",
        "  # Downsampling through the model\n",
        "  skips = down_stack(inputs)\n",
        "  x = skips[-1]\n",
        "  skips = reversed(skips[:-1])\n",
        "\n",
        "  # Upsampling and establishing the skip connections\n",
        "  for up, skip in zip(up_stack, skips):\n",
        "    x = up(x)\n",
        "    concat = tf.keras.layers.Concatenate()\n",
        "    x = concat([x, skip])\n",
        "\n",
        "  # This is the last layer of the model\n",
        "  last = tf.keras.layers.Conv2DTranspose(\n",
        "      filters=output_channels, kernel_size=3, strides=2,\n",
        "      padding='same')  #64x64 -> 128x128\n",
        "\n",
        "  x = last(x)\n",
        "\n",
        "  return tf.keras.Model(inputs=inputs, outputs=x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRsjdZuEnZfA"
      },
      "source": [
        "Tenga en cuenta que la cantidad de filtros en la última capa se establece en la cantidad de `output_channels`. Este será un nodo de salida por clase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0DGH_4T0VYn"
      },
      "source": [
        "## Entrenamiento del modelo\n",
        "\n",
        "Ahora, todo lo que queda por hacer es compilar y entrenar el modelo.\n",
        "\n",
        "Dado que este es un problema de clasificación multiclase, usamos la función de pérdida `tf.keras.losses.CategoricalCrossentropy` con el argumento `from_logits` establecido en `True`, ya que las etiquetas son números enteros escalares en lugar de vectores de puntajes para cada píxel de cada clase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6he36HK5uKAc"
      },
      "outputs": [],
      "source": [
        "OUTPUT_CLASSES = 3\n",
        "\n",
        "model = unet_model(output_channels=OUTPUT_CLASSES)\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVMzbIZLcyEF"
      },
      "source": [
        "Graficamos la arquitectura del modelo resultante:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sw82qF1Gcovr"
      },
      "outputs": [],
      "source": [
        "tf.keras.utils.plot_model(model, show_shapes=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Al momento de realizar la inferencia, la etiqueta asignada a un píxel es el nodo (o canal) con el valor más alto. Esta asignación la realiza la función `create_mask()`.\n",
        "\n",
        "La función `show_predictions()` muestra la imagen de entrada, la máscara groundtruth (i.e. la máscara etiquetada por un experto) y la predicción realizada por el modelo."
      ],
      "metadata": {
        "id": "Y_OnjJyft7pE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwvIKLZPtxV_"
      },
      "outputs": [],
      "source": [
        "def create_mask(pred_mask):\n",
        "  pred_mask = tf.math.argmax(pred_mask, axis=-1)\n",
        "  pred_mask = pred_mask[..., tf.newaxis]\n",
        "  return pred_mask[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLNsrynNtx4d"
      },
      "outputs": [],
      "source": [
        "def show_predictions(dataset=None, num=1):\n",
        "  if dataset:\n",
        "    for image, mask in dataset.take(num):\n",
        "      pred_mask = model.predict(image)\n",
        "      display([image[0], mask[0], create_mask(pred_mask)])\n",
        "  else:\n",
        "    display([sample_image, sample_mask,\n",
        "             create_mask(model.predict(sample_image[tf.newaxis, ...]))])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tc3MiEO2twLS"
      },
      "source": [
        "Probamos el modelo para comprobar lo que predice antes del entrenamiento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_1CC0T4dho3"
      },
      "outputs": [],
      "source": [
        "show_predictions()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22AyVYWQdkgk"
      },
      "source": [
        "La función `DisplayCallback()` definida a continuación se usa para visualizar cómo se comporta la función de pérdida del modelo mientras se está entrenando:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHrHsqijdmL6"
      },
      "outputs": [],
      "source": [
        "class DisplayCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    clear_output(wait=True)\n",
        "    show_predictions()\n",
        "    print ('\\nSample Prediction after epoch {}\\n'.format(epoch+1))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejecutamos el entrenamiento del modelo."
      ],
      "metadata": {
        "id": "sk9QfWfdvjuD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StKDH_B9t4SD"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 20\n",
        "VAL_SUBSPLITS = 5\n",
        "VALIDATION_STEPS = info.splits['test'].num_examples//BATCH_SIZE//VAL_SUBSPLITS\n",
        "\n",
        "model_history = model.fit(train_batches, epochs=EPOCHS,\n",
        "                          steps_per_epoch=STEPS_PER_EPOCH,\n",
        "                          validation_steps=VALIDATION_STEPS,\n",
        "                          validation_data=test_batches,\n",
        "                          callbacks=[DisplayCallback()])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En aras de ahorrar tiempo, el número de épocas se mantuvo pequeño, pero puede configurarlo más alto para lograr resultados más precisos."
      ],
      "metadata": {
        "id": "xsm3O_FyvYmK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_mu0SAbt40Q"
      },
      "outputs": [],
      "source": [
        "loss = model_history.history['loss']\n",
        "val_loss = model_history.history['val_loss']\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(model_history.epoch, loss, 'r', label='Training loss')\n",
        "plt.plot(model_history.epoch, val_loss, 'bo', label='Validation loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss Value')\n",
        "plt.ylim([0, 1])\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unP3cnxo_N72"
      },
      "source": [
        "## Inferencia sobre nuevas imágenes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BVXldSo-0mW"
      },
      "source": [
        "Ahora, vamos a hacer algunas predicciones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikrzoG24qwf5"
      },
      "outputs": [],
      "source": [
        "show_predictions(test_batches, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgiWQh4WPbDL"
      },
      "source": [
        "---\n",
        "\n",
        "# Trabajo Práctico 2 (segunda parte - Opcional)\n",
        "\n",
        "**Acá tienen que dejar los datos de las y los integrantes del grupo:**\n",
        "\n",
        "Nombre y Apellido, DNI, correo eletrónico\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2j3DS0bPOXJd"
      },
      "source": [
        "## **EJERCICIO 5.1**: segmentar nuevas imágenes de mascotas (perros y gatos) con el modelo previamente entrenado\n",
        "\n",
        "1. **Crear un nuevo dataset** propio con un mínimo de **10 imágenes**, donde cada una contenga **al menos 2 mascotas** en escena. Pueden capturar imágenes de sus propias mascotas o buscarlas en la web. Por ejemplo:\n",
        "![](https://assets.jumpseller.com/store/pets-center/themes/476474/options/69738368/flying-with-pets-og-image-1200x630-1.jpeg?1651702172)\n",
        "2. **Predecir las máscaras de segmentación** de cada imagen.\n",
        "\n",
        "TIP: reutilice las celdas de código presentadas anteriormente\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uR0kdgZLV73x"
      },
      "source": [
        "# EJERCICIO 1.1 (OPCIONAL)\n",
        "# ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMv1VZqcUWGQ"
      },
      "source": [
        "## **EJERCICIO 5.2**: desarrollar un enfoque para la tarea de **instance segmentation**\n",
        "\n",
        "A partir de las habilidades y conocimientos adquiridos en el Lab3 y 4, ahora es posible combinar el modelo de detección de objetos con el modelo de segmentación semántica para desarrollar un enfoque de segmentación de instancias. Por ejemplo, usar un modelo para detectar perros y gatos en una imágen y sobre la región de cada detección aplicar un modelo de segmentación (o viceversa). La combinación de ambas produce un segmentación de instancias. Estrictamente hablando, este enfoque no es un modelo por si mismo, sino la concatenación de dos modelos diferentes.\n",
        "\n",
        "Una alternativa es utilizar un único modelo cuya arquitectura fue pensada para la tarea de segmentación de instancias. Existen una gran variedad de modelos que pueden reutilizar para esto. Uno de las más conocidas en Mask R-CNN. En [este artículo](https://towardsdatascience.com/computer-vision-instance-segmentation-with-mask-r-cnn-7983502fcad1) se explora Mask R-CNN para comprender su funcionamiento y como implementarlo usando Keras.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1f36lnOV-RB"
      },
      "source": [
        "# EJERCICIO 1.2 (OPCIONAL)\n",
        "# ..."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}